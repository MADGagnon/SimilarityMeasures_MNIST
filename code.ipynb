{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "IFT3700_Science_des_donnees_Travail_1_Code",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ7Ezx1sug_V",
        "colab_type": "text"
      },
      "source": [
        "# IFT3700 - Science des donnees - Travail 1\n",
        "\n",
        "## Travail présenté à M. Arnaud L'Heureux et M. Alain Tapp\n",
        "\n",
        "## Auteurs :\n",
        "\n",
        "### Ludovic Tuncay (20174139)\n",
        "\n",
        "### Sobhan Mohammadpour (20166971)\n",
        "\n",
        "### Thomas Lavend'homme (20165141)\n",
        "\n",
        "### Marc-Antoine Dufresne Gagnon (20019871)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9nh7qVezSkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install https://github.com/scikit-learn-contrib/scikit-learn-extra/archive/master.zip\n",
        "!pip3 install livelossplot > /dev/null\n",
        "!pip3 install scikit-bio\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.layers import *\n",
        "from tqdm.auto import tqdm\n",
        "from itertools import count\n",
        "from keras.callbacks import *\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import imgaug.augmenters as iaa\n",
        "from keras.datasets import mnist\n",
        "from sklearn.manifold import Isomap\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.optimizers import SGD, Adam\n",
        "from skbio.stats.ordination import pcoa\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from livelossplot.keras import PlotLossesCallback\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.metrics import v_measure_score, silhouette_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqaYXcXfPkk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_knn(k, x_train, y_train, x_test, y_test, mtc='minkowski'):\n",
        "  print(\"score_knn\")\n",
        "  clf = KNeighborsClassifier(k, metric=mtc).fit(x_train, y_train)\n",
        "  score = clf.score(x_test, y_test)\n",
        "  pred = clf.predict(x_test)\n",
        "  print('score = ', score,'v_score = ', v_measure_score(y_test, pred))\n",
        "\n",
        "def score_k_medoids(n_cluster, x_test, y_test, mtc='minkowski'):\n",
        "  print(\"score_k_medoids\")\n",
        "  clf = KMedoids(n_clusters=n_cluster, init='k-medoids++',  metric=mtc)\n",
        "  pred = clf.fit_predict(x_test)\n",
        "  print('v_score = ', v_measure_score(y_test, pred))\n",
        "\n",
        "def generate_pca(x_train, y_train):\n",
        "  print(\"generate_pca\")\n",
        "  pca_new = PCA(n_components=2)\n",
        "  pca_new = pca.fit_transform(x_train)\n",
        "  print('silhouette_score = ',silhouette_score(pca_new,y_train))\n",
        "\n",
        "  # PCA GRAPH\n",
        "  plt.scatter(pca_new[:,0],pca_new[:,1],c=y_train)\n",
        "  plt.xlabel('PC1')\n",
        "  plt.ylabel('PC2')\n",
        "  plt.plot()\n",
        "\n",
        "  # histogram of pca1 in relation to the number of elements for each values\n",
        "  # hist, bin_edges = np.histogram(pca_new, density=True, normed=True)\n",
        "  # plt.hist(pca_new, bins='auto')\n",
        "  # plt.ylabel('number of elements')\n",
        "  # plt.xlabel('PC1')\n",
        "  # plt.title('Histogrram of PCA applied to my similarity space')\n",
        "  # plt.show()\n",
        "\n",
        "def isomap_score(x_test, y_test, n_neighs=10, n_comps=2):\n",
        "  print(\"isomap_score\")\n",
        "  clf = Isomap(n_neighbors = n_neighs, n_components = n_comps)\n",
        "  pred = clf.fit_transform(x_test)\n",
        "  print('silhouette_score = ',silhouette_score(pred,y_test))\n",
        "  plt.scatter(pred[:, 0], pred[:, 1], c=y_test)\n",
        "  plt.plot()\n",
        "\n",
        "def binary_partition_score(n_cluster, x_train, y_train, x_test, y_test, afty='euclidean'):\n",
        "  print(\"binary_partition_score\")\n",
        "  clf = AgglomerativeClustering(n_cluster, linkage='average', affinity=afty)\n",
        "  pred = clf.fit_predict(x_train)\n",
        "  print('v_score =',v_measure_score(pred, y_train))\n",
        "  print('silhouette_score = ',silhouette_score(x_test, y_test))\n",
        "\n",
        "def pcoa_score(x_test, y_test, mtc='optional'):\n",
        "  print(\"binary_partition_score\")\n",
        "  if mtc == 'optional':\n",
        "    dist = cdist(x_test, x_test)\n",
        "  else:\n",
        "    dist = cdist(x_test, x_test, metric=mtc)\n",
        "  res = pcoa(dist, number_of_dimensions=2)\n",
        "  pcoa_new = res.samples.to_numpy()\n",
        "  print('silhouette_score = ',silhouette_score(pcoa_new, y_test))\n",
        "  plt.scatter(pcoa_new[:, 0], pcoa_new[:, 1], c=y_test)\n",
        "  plt.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTJUETF6zSkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading and reshaping mnist data\n",
        "(x_train_original, y_train), (x_test_original, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = np.reshape(x_train_original,(x_train_original.shape[0],28*28))\n",
        "x_test = np.reshape(x_test_original,(x_test_original.shape[0],28*28))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ihkXbSdeLL",
        "colab_type": "text"
      },
      "source": [
        "# Distance Euclidienne"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDHAmAeodVEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Distance Euclidienne\")\n",
        "\n",
        "# KNN\n",
        "score_knn(6, x_train, y_train, x_test[:2000], y_test[:2000])\n",
        "\n",
        "# K medoids\n",
        "score_k_medoids(10, x_train[:20000], y_train[:20000])\n",
        "\n",
        "# PCA\n",
        "generate_pca(x_train, y_train)\n",
        "\n",
        "# Isomap\n",
        "isomap_score(x_train[:5000], y_train[:5000])\n",
        "\n",
        "# Binary Partition\n",
        "binary_partition_score(10, x_train[:10000], y_train[:10000], x_test[:2000], y_test[:2000])\n",
        "\n",
        "# PCoA\n",
        "pcoa_score(x_train[:5000], y_train[:5000])\n",
        "\n",
        "\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsruLabWhZlE",
        "colab_type": "text"
      },
      "source": [
        "# Similarité cosinus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUR7bH-QhYgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = x_train.reshape((-1, 28 * 28))\n",
        "test_x = x_test.reshape((-1, 28 * 28))\n",
        "\n",
        "print(\"Similarité cosinus\")\n",
        "\n",
        "# KNN\n",
        "score_knn(1, train_x, y_train, test_x, y_test, mtc='cosine')\n",
        "\n",
        "# K medoids\n",
        "score_k_medoids(40, train_x[:2000], y_train[:2000], mtc='cosine')\n",
        "\n",
        "# Binary Partition\n",
        "binary_partition_score(100, test_x[:2000], y_test[:2000], test_x[:2000], y_test[:2000], afty='cosine')\n",
        "\n",
        "# PCoA\n",
        "pcoa_score(train_x, y_train, mtc='cosine')\n",
        "\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOF6Z2s4dgyN",
        "colab_type": "text"
      },
      "source": [
        "# Siamese Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TeJW4Sgdmls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Initiation Siamese Net ###\n",
        "\n",
        "## Definition\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE=256\n",
        "HIDDEN_DIM=10\n",
        "\n",
        "seq = iaa.Sequential([\n",
        "    iaa.Affine(\n",
        "        translate_percent={'x': (-0.20, 0.20), 'y':(-0.2, 0.2)}, \n",
        "        rotate=(-35, 35),\n",
        "        shear=(-20, 20),\n",
        "    )\n",
        "])\n",
        "\n",
        "def generator(x, y):\n",
        "  while True:\n",
        "    x1s = []\n",
        "    x2s = []\n",
        "    ys = []\n",
        "    percat=int(len(x)/20)\n",
        "    for i in range(10):\n",
        "      same = np.where(y == i)[0]\n",
        "      x1 = x[same[np.random.choice(len(same), size=percat)]]\n",
        "      x1s.append(x1)\n",
        "\n",
        "      x2 = x[same[np.random.choice(len(same), size=percat)]]\n",
        "      x2s.append(x2)\n",
        "\n",
        "      x1 = x[same[np.random.choice(len(same), size=percat)]]\n",
        "      x1s.append(x1)\n",
        "\n",
        "\n",
        "      other = np.where(y != i)[0]\n",
        "      x2 = x[other[np.random.choice(len(other), size=percat)]]\n",
        "      x2s.append(x2)\n",
        "\n",
        "      t = np.full((2 * percat,), 0)\n",
        "      t[:percat] = 1\n",
        "      ys.append(t)\n",
        "    x1s=np.concatenate(x1s)\n",
        "    x2s=np.concatenate(x2s)\n",
        "    ys=np.concatenate(ys)\n",
        "    shuffle =np.arange(len(ys))\n",
        "    np.random.shuffle(shuffle)\n",
        "    x1s=x1s[shuffle]\n",
        "    x2s=x2s[shuffle]\n",
        "    ys=ys[shuffle]\n",
        "    for i in range(int(len(y)/BATCH_SIZE)):\n",
        "      begin= i * BATCH_SIZE\n",
        "      end = (i+1) *BATCH_SIZE\n",
        "      \n",
        "      r1 = seq(images=x1s[begin: end]).astype(np.float32) / 256\n",
        "      r2 = seq(images=x2s[begin: end]).astype(np.float32) / 256\n",
        "      y1 = ys[begin: end].astype(np.float32)\n",
        "      if np.any(np.isnan(r1)) or np.any(np.isnan(r2)) or np.any(np.isnan(y1)):\n",
        "        print(begin, end)  \n",
        "        print('found nan')\n",
        "      yield [r1, r2], y1\n",
        "\n",
        "\n",
        "# make transformer\n",
        "inputs = Input((28, 28, 1))\n",
        "x = inputs\n",
        "for i in range(5):\n",
        "  x = Conv2D(2**i, (3, 3), activation='relu')(x)\n",
        "x = Flatten()(x)\n",
        "for i in range(5):\n",
        "  x = Dense(100, activation='relu')(x)\n",
        "x = Dense(HIDDEN_DIM, activation='relu')(x)\n",
        "transformer = Model(inputs=inputs, outputs=x)\n",
        "\n",
        "a = Input((28, 28, 1))\n",
        "b = Input((28, 28, 1))\n",
        "aa = transformer(a)\n",
        "bb = transformer(b)\n",
        "def f(x):\n",
        "  a, b = x\n",
        "  res = K.sum(K.square(a - b), axis=1, keepdims=True)\n",
        "  res = K.sqrt(K.maximum(res, 2 * K.epsilon()))\n",
        "  print(res)\n",
        "  return res\n",
        "d = Lambda(f, name='distance_layer')([aa, bb])\n",
        "# ar = inverse(aa)\n",
        "# br = inverse(bb)\n",
        "\n",
        "siamese = Model(inputs=[a, b], output=d)\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    '''Contrastive loss from Hadsell-et-al.'06\n",
        "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    '''\n",
        "    margin = 1\n",
        "    square_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)/2\n",
        "def accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
        "from keras.losses import mean_squared_error\n",
        "siamese.compile(optimizer='adam', loss=contrastive_loss, \n",
        "                metrics={'distance_layer': accuracy})\n",
        "\n",
        "## Entrainement modele\n",
        "\n",
        "train_x = x_train.reshape((-1, 28, 28, 1))\n",
        "test_x = x_test.reshape((-1, 28, 28, 1))\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_x, y_train, test_size=0.2)\n",
        "\n",
        "train_gen = generator(train_x, train_y)\n",
        "val_gen = generator(val_x, val_y)\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True,),\n",
        "]\n",
        "siamese.fit_generator(train_gen, int(len(train_y)/BATCH_SIZE), epochs=EPOCHS, callbacks=callbacks, \n",
        "                      validation_data=val_gen, validation_steps=int(len(val_y)/BATCH_SIZE))\n",
        "\n",
        "## predictions\n",
        "\n",
        "x_train_tr = transformer.predict(train_x)\n",
        "x_test_tr = transformer.predict(test_x)\n",
        "x_val_tr = transformer.predict(val_x)\n",
        "\n",
        "## performance de cette metrique sur les algorithmes de partition\n",
        "\n",
        "print(\"Siamese Net\")\n",
        "\n",
        "siamese.summary()\n",
        "\n",
        "# KNN\n",
        "score_knn(10, x_train_tr, train_y, x_val_tr, val_y)\n",
        "\n",
        "# K medoid\n",
        "score_k_medoids(10, x_val_tr, val_y)\n",
        "\n",
        "# Isomap\n",
        "isomap_score(x_test_tr, y_test)\n",
        "\n",
        "# Binary Partition\n",
        "binary_partition_score(25, x_val_tr, val_y, x_test_tr, y_test)\n",
        "\n",
        "# PCoA\n",
        "pcoa_score(x_test_tr, y_test)\n",
        "\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7POUNrXdnfU",
        "colab_type": "text"
      },
      "source": [
        "# Hash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1kbYTv-drvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Initiation Hash ###\n",
        "\n",
        "## Definition\n",
        "\n",
        "default_params = {\n",
        "    'n':500,\n",
        "    'dist_bin_n': 50,\n",
        "    'angle_bin_n': 50,\n",
        "    'dist_max': 2\n",
        "}\n",
        "\n",
        "def hash(sample,\n",
        "         n=default_params['n'],\n",
        "         dist_bin_n=default_params['dist_bin_n'],\n",
        "         angle_bin_n=default_params['angle_bin_n'],\n",
        "         dist_max=default_params['dist_max']):\n",
        "    x, y = np.nonzero(sample)\n",
        "    non_zero = np.array(tuple(zip(x, y)), dtype=np.float)\n",
        "\n",
        "    p1 = non_zero[np.random.choice(len(non_zero), n)]\n",
        "    p2 = non_zero[np.random.choice(len(non_zero), n)]\n",
        "\n",
        "    delta = (p1 - p2).astype(np.float)\n",
        "\n",
        "    # normalized distance removes scale\n",
        "    distances = np.linalg.norm(delta, axis=1)\n",
        "    distances /= np.mean(distances)\n",
        "    distances[distances >= dist_max] = dist_max\n",
        "\n",
        "    # if delta[, 0] is zero it's vertical\n",
        "    angles = np.arctan(delta[:, 1] / delta[:, 0])\n",
        "    angles[delta[:, 0] == 0] = np.pi/2\n",
        "    # remove rotation\n",
        "    angles += -np.mean(angles) + np.pi/2\n",
        "\n",
        "    distance_bins = np.linspace(0, dist_max, num=dist_bin_n)\n",
        "    distances_digit = np.digitize(distances, distance_bins)\n",
        "    distances_digit[distances_digit==dist_bin_n] = dist_bin_n - 1\n",
        "\n",
        "    angle_bins = np.linspace(0, np.pi+0.001, num=angle_bin_n)\n",
        "    angles_digit = np.digitize(angles, angle_bins)\n",
        "    angles_digit[angles_digit == angle_bin_n] -= 1\n",
        "\n",
        "    store = np.zeros((dist_bin_n, angle_bin_n), dtype=np.int)\n",
        "    for x, y in zip(distances_digit, angles_digit):\n",
        "        store[x, y] += 1\n",
        "    store = store / np.sum(store)\n",
        "    return store\n",
        "\n",
        "def make_hash_transformer(**args):\n",
        "    for i in args.keys():\n",
        "        if i not in default_params:\n",
        "            raise Exception('inavlid param: {}'.format(i))\n",
        "    for i in default_params.keys():\n",
        "        if i not in args:\n",
        "            args[i] = default_params[i]\n",
        "\n",
        "    def func(x):\n",
        "        return np.c_[[hash(i, **args).reshape(-1) for i in tqdm(x)]]\n",
        "\n",
        "    return FunctionTransformer(func, validate=False)\n",
        "\n",
        "\n",
        "## transform data\n",
        "tr = make_hash_transformer(angle_bin_n=10, dist_bin_n=10, dist_max=2)\n",
        "x_train_hash = tr.fit_transform(x_train_original)\n",
        "x_test_hash = tr.fit_transform(x_test_original)\n",
        "\n",
        "## performance de cette metrique sur les algorithmes de partition\n",
        "print(\"Hash\")\n",
        "\n",
        "# KNN\n",
        "score_knn(100, x_train_hash, y_train, x_test_hash, y_test)\n",
        "\n",
        "# K medoid\n",
        "score_k_medoids(100, x_test_hash, y_test)\n",
        "\n",
        "# Isomap\n",
        "isomap_score(x_test_hash, y_test, n_neighs=10, n_comps=2)\n",
        "\n",
        "# Binary Partition\n",
        "binary_partition_score(20, x_train_hash, y_train, x_test_hash, y_test)\n",
        "\n",
        "# PCoA\n",
        "pcoa_score(x_test_hash, y_test)\n",
        "\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b7l69c9tVMX",
        "colab_type": "text"
      },
      "source": [
        "# Distance avec d'autre vecteurs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOFO8vMKtVmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = x_train.reshape((-1, 28 * 28))\n",
        "test_x = x_test.reshape((-1, 28 * 28))\n",
        "\n",
        "print(\"Similarité cosinus\")\n",
        "\n",
        "n = 1000\n",
        "select = np.random.choice(len(train_x), n, replace=False)\n",
        "sterotype=x_train[select]\n",
        "new_x_test = cdist(test_x, sterotype)\n",
        "new_x_train = cdist(train_x, sterotype)\n",
        "\n",
        "# KNN\n",
        "score_knn(10, new_x_train, y_train, new_x_test, y_test)\n",
        "\n",
        "# K medoids\n",
        "score_k_medoids(40, new_x_test, y_test)\n",
        "\n",
        "# Isomap\n",
        "isomap_score(new_x_test, y_test, n_neighs=10, n_comps=2)\n",
        "\n",
        "# Binary Partition\n",
        "binary_partition_score(40, new_x_test, y_test, new_x_test, y_test)\n",
        "\n",
        "# PCoA\n",
        "pcoa_score(new_x_test, y_test)\n",
        "\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRjE8nEv1qmb",
        "colab_type": "text"
      },
      "source": [
        "# Similarite avec reseau de neurones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZtSbLtL1vrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Initiation similarite avec reseau de neurones ###\n",
        "\n",
        "## Definition\n",
        "clf = MLPClassifier(hidden_layer_sizes=(128,128,128),\n",
        "                          batch_size=100,\n",
        "                          #verbose=True,\n",
        "                          max_iter=150,\n",
        "                          n_iter_no_change=25,\n",
        "                          random_state=42)\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "# Pickle transformed data to save time\n",
        "# pickle.dump(clf,open('mlp_clf.pickle','wb'))\n",
        "# clf = pickle.load(open('mlp_clf.pickle','rb'))\n",
        "\n",
        "## predictions en utilisant le reseau de neuronnes\n",
        "\n",
        "x_train_new_rn = clf.predict_proba(x_train)\n",
        "x_test_new_rn = clf.predict_proba(x_test)\n",
        "\n",
        "## performance de cette metrique sur les algorithmes de partition\n",
        "\n",
        "print(\"Similarite avec reseau de neurones\")\n",
        "\n",
        "# KNN\n",
        "score_knn(6, x_train_new_rn, y_train, x_test_new_rn[:2000], y_test[:2000])\n",
        "\n",
        "# K medoid\n",
        "score_k_medoids(10, x_train_new_rn[:20000], y_train_new[:20000])\n",
        "\n",
        "# PCA\n",
        "generate_pca(x_train_new_rn, y_train_new)\n",
        "\n",
        "# Isomap\n",
        "isomap_score(x_train_new_rn[:5000], y_train[:5000])\n",
        "\n",
        "# Binary Partition\n",
        "binary_partition_score(10, x_train_new_rn[:10000], y_train[:10000], x_test_new_rn[:2000], y_test[:2000])\n",
        "\n",
        "# PCoA\n",
        "pcoa_score(x_train_new_rn[:5000], y_train[:5000])\n",
        "\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XC_WVPx1TBf",
        "colab_type": "text"
      },
      "source": [
        "## Invariance aux modifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwoM99Gx1TBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similarity(x_1,x_2) :\n",
        "    probas_x_1,probas_x_2 = clf.predict_proba([x_1, x_2])\n",
        "    #print(probas_x_1,probas_x_2)\n",
        "    return np.linalg.norm(probas_x_1-probas_x_2)\n",
        "\n",
        "def show_image (x,y):\n",
        "    plt.imshow(x.reshape((28,28)))\n",
        "    plt.title(str(y))\n",
        "    plt.show()\n",
        "\n",
        "seq = iaa.Sequential([\n",
        "    iaa.Affine(\n",
        "        translate_percent={'x': (-0.10, 0.10), 'y':(-0.1, 0.1)}, \n",
        "        rotate=(-35, 35),\n",
        "        shear=(-10, 10),\n",
        "    )\n",
        "])\n",
        "\n",
        "rts = seq(images=x_train[:25].reshape((x_train[:25].shape[0],28,28)))\n",
        "rts = rts.reshape((rts.shape[0],28*28))\n",
        "\n",
        "show_image(rts[7],'3 with modifications')\n",
        "show_image(x_train[7],'original 3')\n",
        "show_image(x_train[12],'other 3')\n",
        "show_image(x_train[17],'an original 8')\n",
        "\n",
        "print(\"sim entre 3 mod et 3 original :\",similarity(rts[7],x_train[7]))\n",
        "print(\"sim entre 3 original et autre 3 :\",similarity(x_train[7],x_train[12]))\n",
        "print(\"sim entre 3 mod et autre 3 :\",similarity(rts[7],x_train[12]))\n",
        "print(\"sim entre 3 mod et 8 : \",similarity(rts[7],x_train[17]))\n",
        "print(\"sim entre 3 original et 8 :\",similarity(x_train[7],x_train[17]))\n",
        "print(\"sim entre autre 3 et 8 :\",similarity(x_train[12],x_train[17]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}