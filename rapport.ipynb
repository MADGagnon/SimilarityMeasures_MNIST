{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IFT3700_Science_des_donnees_Travail_1_Rapport",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P_WA1JBiA8Y",
        "colab_type": "text"
      },
      "source": [
        "# IFT3700 - Science des donnees - Travail 1\n",
        "\n",
        "## Travail présenté à M. Arnaud L'Heureux et M. Alain Tapp\n",
        "\n",
        "## Auteurs :\n",
        "\n",
        "### Ludovic Tuncay (20174139)\n",
        "\n",
        "### Sobhan Mohammadpour (20166971)\n",
        "\n",
        "### Thomas Lavend'homme (20165141)\n",
        "\n",
        "### Marc-Antoine Dufresne Gagnon (20019871)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlBAhNUwhAyi",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0m7aNjbZll1",
        "colab_type": "text"
      },
      "source": [
        "Pour ce projet, nous avons été introduits à l’ensemble de données MNIST, soit un ensemble de 60000 échantillons d’entrainement et 10000 échantillons de test. Chacun de ces échantillons correspondent à un tableau à 2 dimensions de 28 par 28 contenants les pixels d’images de chiffres écrits à la main. Le but de ce projet était d’améliorer la performance de différents algorithmes de partition sur cet ensemble en trouvant une meilleure notion de similarité et la comparer à la distance euclidienne. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FiM9eLh04aG",
        "colab_type": "text"
      },
      "source": [
        "# Notions de similarité\n",
        "\n",
        "## Similarité cosinus\n",
        "\n",
        "### Méthodologie\n",
        "\n",
        "Nous créons la matrice de similarité en considérant chaque point comme des vecteurs et calculant l'angle entre eux. Nous obtenons ce résultat en calculant le produit scalaire de chaque paire de points et divisant par le produit de leur norme. Cette métrique est rapide, mais ne fonctionne pas que  les ensembles de données qui ont des valeurs négatives. Lorsque nous avons beaucoup de données cette métrique devient résistant aux petits changements.\n",
        "\n",
        "La fonction utilisée est la suivante :  \n",
        "\n",
        "$$ cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{||\\vec{a}|| ||\\vec{b}||} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNKogxsNZiI8",
        "colab_type": "text"
      },
      "source": [
        "Nous comparons dans ce rapport plusieurs méthodes, soit similarité cosinus, siamese net, hash, distance avec vecteurs et probabilitées avec réseau de neurones  dans le but de trouver la meilleure notion de similarité possible pour ce projet. Finalement, nous avons sélectionné la notion similarité cosinus. Nous élaborerons plus dans la section conclusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACfEPBKz-C4Y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "### Résultats\n",
        "\n",
        "- KNN 1 neighbors\n",
        "  - Accuracy : 0.97\n",
        "  - v_measure : 0.92\n",
        "- KMedoids 40 cluster\n",
        "  - v_measure : 0.53\n",
        "- Partition Binaire 100 cluster\n",
        "  - v_measure : 0.64\n",
        "- PCoA\n",
        "  - Silhouette score: 0.00912\n",
        "\n",
        "![](https://i.imgur.com/Mstqeim.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9HbnuD502UO",
        "colab_type": "text"
      },
      "source": [
        "## Siamese net\n",
        "\n",
        "### Méthodologie\n",
        "\n",
        "Il faut trouver une transformation qui a la propriété suivante : si deux points de données sont près dans l'espace initial, alors leurs images respectives après la transformation seront proches, selon une distance euclidienne. On utilise un Convolutional Neutral Network afin de trouver une telle transformation. Ensuite, nous appliquons le `contrastive loss` sur la distance euclidienne pour entraîner le réseau.\n",
        "\n",
        "### Résultats\n",
        "\n",
        "- KNN 10 neighbors\n",
        "  - Accuracy : 0.8595\n",
        "  - v_measure : 0.727\n",
        "- KMedoids 10 clusters\n",
        "  - v_measure : 0.545\n",
        "- Partition Binaire 25 clusters\n",
        "  - v_measure : 0.50\n",
        "- PCoA\n",
        "  - Silhouette score: 0.030"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgFmW0LRJoGU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "![](https://i.imgur.com/T27lc91.png)\n",
        "- Isomap\n",
        "  - Silhouette score: 0.032\n",
        "\n",
        "![](https://i.imgur.com/Ac5gL4L.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg2dK8JzKfBN",
        "colab_type": "text"
      },
      "source": [
        "## Hash\n",
        "\n",
        "### Méthodologie\n",
        "\n",
        "Nous prenons une série de points aléatoires sur chacun des chiffres à comparer. On fait ensuite la norme et l'angle (par rapport à l'horizontale) du vecteur entre chaque point deux à deux. La transformation est une fonction de la longueur du vecteur, divisé par la moyenne de longueur de tous les vecteurs afin d'être insensible à l'échelle, ainsi qu'à l'angle trouvé, en soustrayant la moyenne de tous les angles, afin d'être insensible à la rotation originelle de l'image.\n",
        "\n",
        "La fonction utilisée est la suivante :  \n",
        "\n",
        "$A - \\overline{A} + \\frac{\\pi}{2}$\n",
        "\n",
        "où $A$ est l'angle du vecteur et $\\overline{A}$ est la moyenne."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1U0f-EG00bS",
        "colab_type": "text"
      },
      "source": [
        "### Résultats\n",
        "\n",
        "L'avantage de cette méthode est sa rapidité d'exécution. De plus, elle est facile à ajuster en fonction de la précision souhaitée.\n",
        "\n",
        "Cela dit, cette méthode n'est pas très efficace. En effectuant KNN avec cette transformation, nous obtenons une précision d'environ 64,5%, alors que KNN avec la distance cosinus obtient une précision nettement meilleure d'environ 95%.\n",
        "\n",
        "- KNN 100 neighbors\n",
        "  - Accuracy : 0.70\n",
        "  - v_measure : 0.53\n",
        "- KMedoids 100 clusters\n",
        "  - v_measure : 0.23\n",
        "- Partition Binaire 20 clusters\n",
        "  - v_measure : 0.2267\n",
        "- PCoA\n",
        "  - silhouette score: -0.0439"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxw_-lAOzjYj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "![](https://i.imgur.com/ND0G8FL.png)\n",
        "- Isomap\n",
        "  - silhouette score: -0.0557\n",
        "\n",
        "![](https://i.imgur.com/CNeqWqw.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lwh5JOC0yoA",
        "colab_type": "text"
      },
      "source": [
        "### Distance avec d'autre vecteurs\n",
        "\n",
        "Comme la distance euclidienne entre les images fonctionne bien, nous pouvons utiliser la distance entre plusieurs stéréotypes comme une transformation. Cette méthode est flexible parce que nous pouvons choisir le nombre de dimension que nous avons (1000 dans notre cas). De plus, la méthode est rapide et simple à calculer. Grâce à la variété des données dans le jeu de données, de légères translations n'influencent pas les résultats, car il existera une autre image similaire dans les données initiales.\n",
        "\n",
        "### Résultats\n",
        "\n",
        "- KNN 10 neighbors\n",
        "  - Accuracy : 0.9228\n",
        "  - v_measure : 0.90\n",
        "- KMedoids 20 clusters\n",
        "  - v_measure : 0.45\n",
        "- Partition Binaire 40 cluster\n",
        "  - v_measure : 0.489\n",
        "- PCoA\n",
        "  - silhouette score: -0.02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi3qrst2Lqa_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "![](https://i.imgur.com/sj1bYL3.png)\n",
        "- Isomap\n",
        "  - silhouette score: 0.03\n",
        "\n",
        "![](https://i.imgur.com/9mtMHxE.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kj1kkShKzeE",
        "colab_type": "text"
      },
      "source": [
        "## Similarité avec un réseau de neurones\n",
        "\n",
        "### Méthodologie\n",
        "\n",
        "L'idée de cette similarité est d'utiliser un réseau de neurones pour déterminer si deux éléments sont similaires. Pour ce faire on va entrainer un MLP (mulit-layer perceptron) qui nous donnera en sortie un vecteur de probabilités. Ce vecteur correspondra aux probabilités que l'image appartienne aux différentes classes. Grâce à cela on définit la similarité entre deux images comme étant la distance euclidienne entre ces vecteurs de prédictions.\n",
        "Pour utiliser cette distance nous avons deux possibilités :\n",
        "1. Donner la fonction de similarité directement à l'algorithme (dans KNN  par exemple)\n",
        "2. Transformer chaque donnée en entrée par son vecteur donné par le MLP. Ensuite, nous appliquons les algorithmes sur ces \"nouveaux\" points.\n",
        "  \n",
        "  + Puisque nous appliquons une distance euclidienne après, ces deux méthodes sont équivalentes (étant donné que les algorithmes utilisent la distance euclidienne par défaut)\n",
        "\n",
        "Cette notion de similarité marche très bien et donne de meilleurs résultats que la distance euclidienne dans toutes les catégories (voir tableau ci-dessous).\n",
        "\n",
        "### Résultats\n",
        "\n",
        "        \n",
        "  Temps pour entrainer le MLP |7min 29s|\n",
        "  ----------------------------|--------|\n",
        "\n",
        "\n",
        "\n",
        "modèles ->                  | Knn    |K-medoïds|Clustering|PCA      |Isomap   |PCoA |\n",
        "----------------------------|:------:|:-------:|:--------:|:-------:|:-------:|:---:|\n",
        "***distance euclidienne***  |        |         |          |         |         |\n",
        "temps d'execution (fit)     |19.4 s* |11.9 s*  |48.6 s    |1.67 s   |43.4 s*  |23.5 s*\n",
        "temps d'exécution (predict ou transform) |31.3 s*  |34.9ms   |No result |283 ms   |30.8 s*  |13.9 s*\n",
        "v-measure score ou score silhouette|0.8948 (v measure)|0.3489 (v measure)|0.2346 (v measure)|0.0234 (silhouette)|0.0029 (silhouette)|0.0312 (silhouette)\n",
        "***cette similarité***         |        |         |          |         |         |\n",
        "temps d'execution (fit)     |62.1 ms*|8.15 s*  |3.76 s    |61.3 ms  |3.79 s*  |249 ms*\n",
        "temps d'exécution (predict ou transform) |104 ms*  |4.79 ms  |No result |67.7 ms  |1.22 s* |11.9 s*\n",
        "v-measure score ou score silhouette|0.9283 (v measure) |0.9399  (v measure) |0.9947 (v measure)|0.9564 (silhouette) |0.6118 (silhouette)  |0.9430 (silhouette)\n",
        "* : fait sur un sous ensemble des données |    |          |         |         |\n",
        "\n",
        "<sub><sup><sup><sub> Processeur sur lequel les temps sont mesurés : Processeur Intel Core i5 4 coeurs 2.3 GHz (8259U)</sub></sup></sup></sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsgfeONWz7dS",
        "colab_type": "text"
      },
      "source": [
        "1. PCA : On remarque que pour cette notion de similarité, la plupart des données sont rassemblées _en bas a gauche_. et d'autres données sont éparpillées sur le reste de l'espace. Ce résultat est intuitif, car dans l'espace transformé, les points similaires seront proche des vecteurs unités avec une variance très faible. Ceux-ci seront ainsi tous rassemblé dans une zone en appliquant PCA. Le reste des points sont plus difficiles à catégoriser. Par conséquent, ils seront plus éparpillés sur l'espace réduit final.  \n",
        "\n",
        " ![](https://imgur.com/lGmuSoZ.png) \n",
        " \n",
        " ![](https://imgur.com/eKZrOKL.png)\n",
        "\n",
        "  Pour confirmer cette notion on peut appliquer PCA pour qu'il ne reste qu'une seule dimension. Les données faciles à catégoriser seront rassemblées et les quelques points difficiles à catégoriser seront un peu plus loin dans l'espace. C'est effectivement ce qu'on observe :\n",
        "\n",
        " ![](https://imgur.com/dcmaRtx.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8gDLDkWzxrp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "2. Isomap : \n",
        "\n",
        " ![](https://imgur.com/LBDiV0x.png) \n",
        " \n",
        " ![](https://imgur.com/ryjkvcg.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foRugbfC0us-",
        "colab_type": "text"
      },
      "source": [
        "3. PCoA : \n",
        "\n",
        " ![](https://imgur.com/UqInqyd.png)\n",
        "\n",
        " ![](https://imgur.com/Pvd25RV.png) \n",
        " \n",
        " ![](https://imgur.com/qPHvjLY.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww4FJaQbz_iK",
        "colab_type": "text"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "* Avantages :\n",
        "  - Donne de très bons résultats\n",
        "  - Une fois la transformation appliquée, le temps d'exécution devient beaucoup plus rapide.\n",
        "  - Moins sensible aux petites modifications dans nos données (translations, etc ...)\n",
        "* Désavantages :\n",
        "  - La méthode est supervisée. En effet cette notion de similarité nécessite un ensemble d'entrainement labélisé pour fonctionner.\n",
        "  - Le temps de calcul pris pour entrainer le réseau est assez long.\n",
        "\n",
        "### Remarques :\n",
        "\n",
        "- Cette méthode n'est en fait pas obligatoirement supervisée. En effet il est possible de prendre une autre méthode non supervisée pour obtenir ces vecteurs de probabilités, mais cela complique la tâche. La solution proposée est simplement une version plus facile à implémenter.\n",
        "- Le temps de calcul est surtout pris pour l'entrainement du modèle, en effet une fois que nous \"transformons nos points\" le temps de calcul est plus rapide, car nous manipulons des données à 10 dimensions (versus 784).\n",
        "- Cette notion de similarité est insensible aux modifications du moment que l'image transformée ressemble encore à un chiffre. Cette invariance est illustrée ci-dessous :  \n",
        "\n",
        "distance             | 3 original | 3 avec modifications | autre 3 | 8 original\n",
        "---------------------|:----------:|:--------------------:|:-------:|:---------:\n",
        "3 original           |0           | 5.114e-06            |6.147e-40|1.414\n",
        "3 avec modifications |5.114e-06   |0                     |5.114e-06|1.414\n",
        "autre 3              |6.147e-40   |5.113e-06             |0        |1.414\n",
        "8 original           |1.414       |1.414                 |1.414    |0\n",
        "\n",
        "Voici les differentes images :\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPvwu9qw0C5i",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![](https://imgur.com/gHJP05z.png)\n",
        "\n",
        "![](https://imgur.com/cUG8bkt.png)\n",
        "\n",
        "![](https://imgur.com/vMQ0Aup.png) \n",
        "\n",
        "![](https://imgur.com/QIoB5X2.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-fvSHj80FVQ",
        "colab_type": "text"
      },
      "source": [
        "  On remarque grâce au tableau que la forme influence peu le résultat. Les 3 sont similaires, mais ne le sont pas avec le 8. Cela confirme donc l'invariance de ma notion de similarité aux légères transformations.  \n",
        "  <sup><sub>Remarque : les transformations appliquées ici sont des translations, des rotations et des étirements</sub></sup>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_-zIPvvk4-S",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Pour un jeu de données comme MNIST, puisque nous avons de bons labels (i.e. les données sont correctement labélisées), nous pouvons trouver de bonnes métriques. Par exemple en utilisant les probabilités avec un réseau de neurones (voir partie correspondante) qui donne de très bons résultats. Nous aurions aussi pu directement faire une transformation vers le label prédit (donc en une dimension). Cela donnerai une bonne partition mais l'idée de similarité serait plus abstaite. Cependant deux des gros désavantages de cette idée sont de devoir utiliser des données labelisées (i.e. methode supervisée) mais aussi le temps nécessaire à l'entrainement du modèle.\n",
        "\n",
        "Si nous n'avions pas de labels, le nombre de données joue en notre faveur. En effet MNIST a une variété limitée. De légères transformations, translations ou rotations, n'influencent pas beaucoup les résultats finaux. Ce fait est même vrai pour la métrique \"distance cosinus\", qui n'est pas robuste à ce type de transformations. Ainsi, puisque nous avons accès à cette grande quantité de données, la distance cosinus est la métrique sans supervision avec laquelle nous avons expérimenté, qui est la plus rapide, la plus simple et qui au final a de très bons resultats.\n",
        "\n",
        "Une autre bonne métrique est la distance avec vecteurs, qui est rapide et robuste sur les petites transformations.\n",
        "\n",
        "Contrairement à nos hypothèses initiales, la métrique utilisant le hash n'a pas donné de bons résultats. Nous pensons que cette métrique est plus utile dans une situation avec des données plus éloignées et en moins grande quantité. Ce qui n'est pas le cas avec MNIST.\n",
        "\n",
        "Les techniques de réduction de dimentionalités, comme PCoA et Isomap, n'ont pas générées de graphes avec des partitions distinctes. Nous voyons localement de bonnes partitions visiblement séparés, mais globalement nous n'obtenons pas de bons résultats de silhouettes (silhouette $\\approx$ 0). Excepté la similarité avec réseau de neurones qui donne de très bons scores (voir tableau dans la partie correspondante).\n",
        "\n"
      ]
    }
  ]
}